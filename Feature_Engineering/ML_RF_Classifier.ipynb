{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "717e4f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# sys.path.append('/Users/bernardoloureiro/template-lib')\n",
    "\n",
    "from utils.notebookhelpers.helpers import Helpers\n",
    "from utils.dtos.templateOutputCollection import TemplateOutputCollection\n",
    "from utils.dtos.variable import Metadata\n",
    "from utils.dtos.templateOutput import TemplateOutput\n",
    "from utils.dtos.templateOutput import OutputType\n",
    "from utils.dtos.templateOutput import ChartType\n",
    "import datetime\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from dateutil import parser\n",
    "logging.basicConfig(format='%(levelname)s:%(message)s', level=logging.INFO)\n",
    "import math\n",
    "import scipy as scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import copy\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve, auc\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.calibration import CalibratedClassifierCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30b61e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDatasetParameter=Helpers.get_or_create_input_dataset(\n",
    "    name=\"inputDataset6\",\n",
    "    metadata=Metadata(input_name='Model_Input_Data', is_required=True\n",
    "                      , tooltip='Model Input data for Classification'),\n",
    "    local_context=locals()\n",
    ")\n",
    "\n",
    "\n",
    "outputDatasetParameter=Helpers.get_or_create_output_dataset(\n",
    " name=\"Model_result\",\n",
    "    metadata=Metadata(input_name='Model Accuracy Scores', is_required=True, tooltip='Dataset name to be created after the transformation'),\n",
    "    local_context=locals()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a455505e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the context\n",
    "contextId = 'HealtCare_Fraud_data_ML_Classification'\n",
    "context = Helpers.getOrCreateContext(contextId=contextId, localVars=locals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff49d1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the parameters\n",
    "train_iobp_df_final=inputDatasetParameter.value\n",
    "\n",
    "\n",
    "\n",
    "outputDataset=outputDatasetParameter.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c9f906",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iobp_df_final = Helpers.getEntityData(context, train_iobp_df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44a2c15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_prob(clf, data): \n",
    "    \"\"\"\n",
    "    Description :: This function is created for storing the predicted probabability using the trained model.\n",
    "    \n",
    "    Input :: It accepts below input parameters :\n",
    "      - clf : Trained model classifier\n",
    "      - data : Dataset for which we want to generate the predictions\n",
    "    \"\"\"\n",
    "    y_pred = clf.predict_proba(data)[:,1]\n",
    "    return y_pred\n",
    "\n",
    "def draw_roc(train_fpr, train_tpr, test_fpr, test_tpr):\n",
    "    \"\"\"\n",
    "    Description :: This function is created for calculating the AUC score on train and test data. And, plotting the ROC curve.\n",
    "    \n",
    "    Input :: It accepts below input parameters :\n",
    "      - train_fpr : Train False +ve rate\n",
    "      - train_tpr : Train True +ve rate\n",
    "      - test_fpr : Test False +ve rate\n",
    "      - test_tpr : Test True +ve rate\n",
    "    \"\"\"\n",
    "    # calculate auc for train and test\n",
    "    train_auc = auc(train_fpr, train_tpr)\n",
    "    test_auc = auc(test_fpr, test_tpr)\n",
    "    with plt.style.context('seaborn-poster'):\n",
    "      plt.plot(train_fpr, train_tpr, label=\"Train AUC =\"+\"{:.4f}\".format(train_auc), color='blue')\n",
    "      plt.plot(test_fpr, test_tpr, label=\"Test AUC =\"+\"{:.4f}\".format(test_auc), color='red')\n",
    "      plt.legend()\n",
    "      plt.xlabel(\"False Positive Rate(FPR)\", fontdict=label_font_dict)\n",
    "      plt.ylabel(\"True Positive Rate(TPR)\", fontdict=label_font_dict)\n",
    "      plt.title(\"Area Under Curve\", fontdict=title_font_dict)\n",
    "      plt.grid(b=True, which='major', color='lightgrey', linestyle='--')\n",
    "      plt.minorticks_on()\n",
    "      plt.show()\n",
    "    \n",
    "def find_best_threshold(threshold, fpr, tpr):\n",
    "    \"\"\"\n",
    "    Description :: This function is created for finding the best threshold value.\n",
    "    \"\"\"\n",
    "    t = threshold[np.argmax(tpr * (1-fpr))]\n",
    "    return t\n",
    "\n",
    "def predict_with_best_t(proba, threshold):\n",
    "    \"\"\"\n",
    "    Description :: This function is created for generating the predictions based on the best threshold value.\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    for i in proba:\n",
    "        if i>=threshold:\n",
    "            predictions.append(1)\n",
    "        else:\n",
    "            predictions.append(0)\n",
    "    return predictions\n",
    "\n",
    "def draw_confusion_matrix(best_t, x_train, x_test, y_train, y_test, y_train_pred, y_test_pred):\n",
    "    \"\"\"\n",
    "    Description :: This function is created for plotting the confusion matrix of TRAIN and TEST sets.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(1,2, figsize=(20,6))\n",
    "\n",
    "    train_prediction = predict_with_best_t(y_train_pred, best_t)\n",
    "    cm = confusion_matrix(y_train, train_prediction)\n",
    "    with plt.style.context('seaborn'):\n",
    "        sns.heatmap(cm, annot=True, fmt='d', ax=ax[0], cmap='viridis')\n",
    "        ax[0].set_title('Train Dataset Confusion Matrix', fontdict=title_font_dict)\n",
    "        ax[0].set_xlabel(\"Predicted Label\", fontdict=label_font_dict)\n",
    "        ax[0].set_ylabel(\"Actual Label\", fontdict=label_font_dict)\n",
    "\n",
    "    test_prediction = predict_with_best_t(y_test_pred, best_t)\n",
    "    cm = confusion_matrix(y_test, test_prediction)\n",
    "    with plt.style.context('seaborn'):\n",
    "        sns.heatmap(cm, annot=True, fmt='d', ax=ax[1], cmap='summer')\n",
    "        ax[1].set_title('Test Dataset Confusion Matrix', fontdict=title_font_dict)\n",
    "        ax[1].set_xlabel(\"Predicted Label\", fontdict=label_font_dict)\n",
    "        ax[1].set_ylabel(\"Actual Label\", fontdict=label_font_dict)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return train_prediction, test_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "968401ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(clf, x_train, x_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Description :: This function is created for performing the evaluation of the trained model.\n",
    "    \"\"\"\n",
    "    # predict the probability of train data\n",
    "    y_train_pred = pred_prob(clf, x_train)\n",
    "    \n",
    "    # predict the probability of test data\n",
    "    y_test_pred = pred_prob(clf, x_test)\n",
    "    \n",
    "    # calculate tpr, fpr using roc_curve\n",
    "    train_fpr, train_tpr, tr_thresholds = roc_curve(y_train, y_train_pred)\n",
    "    test_fpr, test_tpr, te_thresholds = roc_curve(y_test, y_test_pred)\n",
    "    \n",
    "    # calculate auc for train and test\n",
    "    train_auc = auc(train_fpr, train_tpr)\n",
    "    print(\"### Train AUC = {}\".format(train_auc))\n",
    "    test_auc = auc(test_fpr, test_tpr)\n",
    "    print(\"### Test AUC = {}\".format(test_auc))\n",
    "    \n",
    "    # plotting the ROC curve\n",
    "    draw_roc(train_fpr, train_tpr, test_fpr, test_tpr)\n",
    "    \n",
    "    # Best threshold value\n",
    "    best_t = find_best_threshold(tr_thresholds, train_fpr, train_tpr)\n",
    "    \n",
    "    # Plotting the confusion matrices\n",
    "    train_prediction, test_prediction = draw_confusion_matrix(best_t, x_train, x_test, y_train, y_test, y_train_pred, y_test_pred)\n",
    "    \n",
    "    # Generating the F1-scores\n",
    "    train_f1_score = f1_score(y_train, train_prediction)\n",
    "    test_f1_score = f1_score(y_test, test_prediction)\n",
    "    \n",
    "    return test_auc, train_f1_score, test_f1_score, best_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a535ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RF_classifier(train_iobp_df_final):\n",
    "    train_iobp_df_final = train_iobp_df_final.groupby(['Provider','PotentialFraud'],as_index=False).agg('sum')\n",
    "    X = train_iobp_df_final.drop(axis=1, columns=['Provider','PotentialFraud'])\n",
    "    y = train_iobp_df_final['PotentialFraud']\n",
    "    X_train, X_test, y_train, y_test = tts(X, y, test_size=0.20, stratify=y, random_state=39)\n",
    "#     X_train, X_test, y_train, y_test = tts(X, y, test_size=0.25, stratify=y, random_state=39)\n",
    "    \n",
    "    # Standardize the data (train and test)\n",
    "    robust_scaler = RobustScaler()\n",
    "    robust_scaler.fit(X_train)\n",
    "    X_train_std = robust_scaler.transform(X_train)\n",
    "    X_test_std = robust_scaler.transform(X_test)\n",
    "    \n",
    "    # Standardize the data (train and test)\n",
    "    robust_scaler = RobustScaler()\n",
    "    robust_scaler.fit(X_train)\n",
    "    X_train_std = robust_scaler.transform(X_train)\n",
    "    X_test_std = robust_scaler.transform(X_test)\n",
    "    \n",
    "    # Performing minority oversampling\n",
    "    oversample = ADASYN(sampling_strategy=0.45, n_neighbors=8)\n",
    "    X_train_ovsamp, y_train_ovsamp = oversample.fit_resample(X_train_std, y_train)\n",
    "    \n",
    "    return X_train_ovsamp, y_train_ovsamp, X_test_std,y_test\n",
    "\n",
    "\n",
    "X_train_ovsamp, y_train_ovsamp, X_test_std,y_test = RF_classifier(train_iobp_df)\n",
    "\n",
    "\n",
    "# counter = Counter(y_train_ovsamp)\n",
    "\n",
    "# fraud_percentage = (counter[1]*100 / (counter[0]+counter[1]))\n",
    "# non_fraud_percentage = (counter[0]*100 / (counter[0]+counter[1]))\n",
    "# print(\"Fraud Percentage = {:.2f}% and Non-Fraud Percentage = {:.2f}%\".format(fraud_percentage, non_fraud_percentage))\n",
    "\n",
    "\n",
    "# Training the model with all features and hyper-parameterized values\n",
    "rfc = RandomForestClassifier(n_estimators=70,criterion='gini',\n",
    "                                   max_depth= 4,\n",
    "                                   max_features='auto',\n",
    "                                   min_samples_leaf=30,\n",
    "                                   min_samples_split=30,\n",
    "                                   random_state=47,\n",
    "                                   min_weight_fraction_leaf=0.0,\n",
    "                                   max_leaf_nodes=None,\n",
    "                                   min_impurity_decrease=0.0,\n",
    "                                   ccp_alpha=0.0)\n",
    "\n",
    "rfc.fit(X_train_ovsamp, y_train_ovsamp)\n",
    "\n",
    "\n",
    "# Validate model\n",
    "test_auc, train_f1_score, test_f1_score, best_t = validate_model(rfc, X_train_ovsamp, X_test_std, y_train_ovsamp, y_test)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"### Best Threshold = {:.4f}\".format(best_t))\n",
    "print(\"### Model AUC is : {:.4f}\".format(test_auc))\n",
    "print(\"### Model Train F1 Score is : {:.4f}\".format(train_f1_score))\n",
    "print(\"### Model Test F1 Score is : {:.4f}\".format(test_f1_score))\n",
    "\n",
    "results = pd.DataFrame(zip([test_auc, train_f1_score, test_f1_score, best_t]),index=['Best Threshold','Best model AUC',\n",
    "                                                                           'Model Train_F1 Score','Model Test_F1 Score'],\n",
    "                                                                             columns=['Results'])\n",
    "\n",
    "# feats_imps = pd.DataFrame({'Features': X_train.columns, 'Importance_Model_1': rfc.feature_importances_})\n",
    "# feats_imps = feats_imps[feats_imps['Importance_Model_1'] != 0]\n",
    "# feats_imps.reset_index(drop=True, inplace=True)\n",
    "# feats_imps.head()\n",
    "\n",
    "# top_20_pos_feats = feats_imps.sort_values(by='Importance_Model_1',axis=0,ascending=False)['Features'].iloc[0:20]\n",
    "# top_20_pos_feats_scores = feats_imps.sort_values(by='Importance_Model_1',axis=0,ascending=False)['Importance_Model_1'].iloc[0:20]\n",
    "\n",
    "\n",
    "# with plt.style.context('seaborn-poster'):\n",
    "#     sns.barplot(y=top_20_pos_feats, x=top_20_pos_feats_scores, orient='h', palette='coolwarm')\n",
    "#     plt.xlabel(\"\\nFeatures Importance\", fontdict=label_font_dict)\n",
    "#     plt.ylabel(\"Features\\n\", fontdict=label_font_dict)\n",
    "#     plt.title(\"Top 15 Importance Positive Features\\n\", fontdict=title_font_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a8fdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputCollection = Helpers.createOutputCollection(context)\n",
    "out = Helpers.createTemplateOutputDataset(context=context, outputName=outputDataset, dataFrame=results)\n",
    "outputCollection.addTemplateOutput(out)\n",
    "Helpers.save(context)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
