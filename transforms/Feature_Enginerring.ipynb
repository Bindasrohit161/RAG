{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f578cac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# sys.path.append('/Users/bernardoloureiro/template-lib')\n",
    "\n",
    "from utils.notebookhelpers.helpers import Helpers\n",
    "from utils.dtos.templateOutputCollection import TemplateOutputCollection\n",
    "from utils.dtos.variable import Metadata\n",
    "from utils.dtos.templateOutput import TemplateOutput\n",
    "from utils.dtos.templateOutput import OutputType\n",
    "from utils.dtos.templateOutput import ChartType\n",
    "import datetime\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from dateutil import parser\n",
    "logging.basicConfig(format='%(levelname)s:%(message)s', level=logging.INFO)\n",
    "import math\n",
    "import scipy as scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a51321d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:Context is not present for given context_id: None. Make sure you declare context_id at the top of the cell and run prepareForLocal in your flow file before testing transform file.\n",
      "WARNING:Context is not present for given context_id: None. Make sure you declare context_id at the top of the cell and run prepareForLocal in your flow file before testing transform file.\n",
      "WARNING:Context is not present for given context_id: None. Make sure you declare context_id at the top of the cell and run prepareForLocal in your flow file before testing transform file.\n",
      "WARNING:Context is not present for given context_id: None. Make sure you declare context_id at the top of the cell and run prepareForLocal in your flow file before testing transform file.\n",
      "WARNING:Context is not present for given context_id: None. Make sure you declare context_id at the top of the cell and run prepareForLocal in your flow file before testing transform file.\n"
     ]
    }
   ],
   "source": [
    "inputDatasetParameter=Helpers.get_or_create_input_dataset(\n",
    "    name=\"inputDataset\",\n",
    "    metadata=Metadata(input_name='train_bene_df', is_required=True, tooltip='Benefeciary Dataset to apply the transformation'),\n",
    "    local_context=locals()\n",
    ")\n",
    "\n",
    "inputDatasetParameter2=Helpers.get_or_create_input_dataset(\n",
    "    name=\"inputDataset2\",\n",
    "    metadata=Metadata(input_name='train_ip_df', is_required=True, tooltip='InPateint Dataset to apply the transformation'),\n",
    "    local_context=locals()\n",
    ")\n",
    "\n",
    "inputDatasetParameter3=Helpers.get_or_create_input_dataset(\n",
    "    name=\"inputDataset3\",\n",
    "    metadata=Metadata(input_name='train_op_df', is_required=True, tooltip='OpPateint Dataset to apply the transformation'),\n",
    "    local_context=locals()\n",
    ")\n",
    "\n",
    "inputDatasetParameter4=Helpers.get_or_create_input_dataset(\n",
    "    name=\"inputDataset4\",\n",
    "    metadata=Metadata(input_name='train_tgt_lbls_df', is_required=True, tooltip='Target Dataset to apply the transformation'),\n",
    "    local_context=locals()\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "outputDatasetParameter=Helpers.get_or_create_output_dataset(\n",
    " name=\"outputDataset\",\n",
    "    metadata=Metadata(input_name='Output Dataset', is_required=True, tooltip='Dataset name to be created after the transformation'),\n",
    "    local_context=locals()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31383aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the context\n",
    "contextId = 'HealtCare_Fraud'\n",
    "context = Helpers.getOrCreateContext(contextId=contextId, localVars=locals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e022e5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the parameters\n",
    "train_bene_df=inputDatasetParameter.value\n",
    "train_ip_df= inputDatasetParameter2.value\n",
    "train_op_df= inputDatasetParameter3.value\n",
    "train_tgt_lbls_df= inputDatasetParameter4.value\n",
    "\n",
    "\n",
    "\n",
    "outputDataset=outputDatasetParameter.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7b000e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entity doesn't exist\n",
      "entity doesn't exist\n",
      "entity doesn't exist\n",
      "entity doesn't exist\n"
     ]
    }
   ],
   "source": [
    "train_bene_df = Helpers.getEntityData(context, train_bene_df)\n",
    "train_ip_df = Helpers.getEntityData(context, train_ip_df)\n",
    "train_op_df = Helpers.getEntityData(context, train_op_df)\n",
    "train_tgt_lbls_df = Helpers.getEntityData(context, train_tgt_lbls_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07efc912",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_Enginerring(train_bene_df,train_ip_df,train_op_df,train_tgt_lbls_df):\n",
    "\n",
    "    train_ip_df[\"Admitted?\"] = 1\n",
    "    train_op_df[\"Admitted?\"] = 0\n",
    "    # Merging the IP and OP dataset on the basis of common columns.\n",
    "    common_cols = [col for col in train_ip_df.columns if col in train_op_df.columns]\n",
    "    train_ip_op_df = pd.merge(left=train_ip_df, right=train_op_df, left_on=common_cols, right_on=common_cols, how=\"outer\")\n",
    "    train_ip_op_bene_df = pd.merge(left=train_ip_op_df, right=train_bene_df, left_on='BeneID', right_on='BeneID',how='inner')\n",
    "    ### **Merging the IP_OP_BENE Dataset with PROVIDER level Tgt Labels Data**\n",
    "    train_iobp_df = pd.merge(left=train_ip_op_bene_df, right=train_tgt_lbls_df, left_on='Provider', right_on='Provider',how='inner')\n",
    "\n",
    "    # Joining with the PRV Tgt Labels\n",
    "    prvs_claims_df = pd.DataFrame(train_iobp_df.groupby(['Provider'])['ClaimID'].count()).reset_index()\n",
    "    prvs_claims_tgt_lbls_df = pd.merge(left=prvs_claims_df, right=train_tgt_lbls_df, on='Provider', how='inner')\n",
    "\n",
    "\n",
    "    #Feature-2\n",
    "    ## Is Alive? = No if DOD is NaN else Yes\n",
    "\n",
    "    train_iobp_df['DOB'] = pd.to_datetime(train_iobp_df['DOB'], format=\"%Y-%m-%d\")\n",
    "    train_iobp_df['DOD'] = pd.to_datetime(train_iobp_df['DOD'], format=\"%Y-%m-%d\")\n",
    "#     train_iobp_df['Is_Alive?'] = train_iobp_df['DOD'].apply(lambda val: 'No' if val != val else 'Yes')\n",
    "\n",
    "    ## Adding New Feature - 3 :: Claim_Duration\n",
    "\n",
    "    #Claim Duration = Claim End Date - Claim Start Date\n",
    "    train_iobp_df['ClaimStartDt'] = pd.to_datetime(train_iobp_df['ClaimStartDt'], format=\"%Y-%m-%d\")\n",
    "    train_iobp_df['ClaimEndDt'] = pd.to_datetime(train_iobp_df['ClaimEndDt'], format=\"%Y-%m-%d\")\n",
    "\n",
    "    train_iobp_df['Claim_Duration'] = (train_iobp_df['ClaimEndDt'] - train_iobp_df['ClaimStartDt']).dt.days\n",
    "\n",
    "    ### **Adding `New Feature - 4` :: `Admitted_Duration`**\n",
    "\n",
    "    #Admitted Duration = Discharge Date - Admission Date\n",
    "\n",
    "    train_iobp_df['AdmissionDt'] = pd.to_datetime(train_iobp_df['AdmissionDt'], format=\"%Y-%m-%d\")\n",
    "    train_iobp_df['DischargeDt'] = pd.to_datetime(train_iobp_df['DischargeDt'], format=\"%Y-%m-%d\")\n",
    "\n",
    "    train_iobp_df['Admitted_Duration'] = (train_iobp_df['DischargeDt'] - train_iobp_df['AdmissionDt']).dt.days\n",
    "\n",
    "\n",
    "    ### **Adding `New Feature - 5` :: `Bene_Age`**\n",
    "\n",
    "    ##Bene Age = DOD - DOB (if DOD is Null then replace it with MAX date in DOD)\n",
    "\n",
    "    # Filling the Null values as MAX Date of Death in the Dataset\n",
    "    train_iobp_df['DOD'].fillna(value=train_iobp_df['DOD'].max(), inplace=True)\n",
    "    train_iobp_df['Bene_Age'] = round(((train_iobp_df['DOD'] - train_iobp_df['DOB']).dt.days)/365,1)\n",
    "\n",
    "    ### **Adding `New Feature - 6` :: `Att_Opr_Oth_Phy_Tot_Claims`**\n",
    "\n",
    "\n",
    "    train_iobp_df['Att_Phy_tot_claims'] = train_iobp_df.groupby(['AttendingPhysician'])['ClaimID'].transform('count')\n",
    "    train_iobp_df['Opr_Phy_tot_claims'] = train_iobp_df.groupby(['OperatingPhysician'])['ClaimID'].transform('count')\n",
    "    train_iobp_df['Oth_Phy_tot_claims'] = train_iobp_df.groupby(['OtherPhysician'])['ClaimID'].transform('count')\n",
    "\n",
    "    # Creating the combined feature\n",
    "    train_iobp_df['Att_Phy_tot_claims'].fillna(value=0, inplace=True)\n",
    "    train_iobp_df['Opr_Phy_tot_claims'].fillna(value=0, inplace=True)\n",
    "    train_iobp_df['Oth_Phy_tot_claims'].fillna(value=0, inplace=True)\n",
    "\n",
    "    train_iobp_df['Att_Opr_Oth_Phy_Tot_Claims'] = train_iobp_df['Att_Phy_tot_claims'] + train_iobp_df['Opr_Phy_tot_claims'] + train_iobp_df['Oth_Phy_tot_claims']\n",
    "    train_iobp_df.drop(['Att_Phy_tot_claims', 'Opr_Phy_tot_claims', 'Oth_Phy_tot_claims'], axis=1, inplace=True)\n",
    "\n",
    "   \n",
    "\n",
    "    train_iobp_df[\"Prv_Tot_Att_Phy\"] = train_iobp_df.groupby(['Provider'])['AttendingPhysician'].transform('count')\n",
    "    train_iobp_df[\"Prv_Tot_Opr_Phy\"] = train_iobp_df.groupby(['Provider'])['OperatingPhysician'].transform('count')\n",
    "    train_iobp_df[\"Prv_Tot_Oth_Phy\"] = train_iobp_df.groupby(['Provider'])['OtherPhysician'].transform('count')\n",
    "\n",
    "    train_iobp_df['Prv_Tot_Att_Opr_Oth_Phys'] = train_iobp_df['Prv_Tot_Att_Phy'] + train_iobp_df['Prv_Tot_Opr_Phy'] + train_iobp_df['Prv_Tot_Oth_Phy']\n",
    "    train_iobp_df.drop(['Prv_Tot_Att_Phy', 'Prv_Tot_Opr_Phy', 'Prv_Tot_Oth_Phy'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "    ### **Adding `New Feature - 8` :: `Total Unique Claim Admit Codes used by a PROVIDER`**\n",
    "\n",
    "    # * **`Reasoning`** :: The idea behind adding this feature is to see how many unique number of `Claim Admit Diagnosis Codes` used by the Provider. \n",
    "    #        * As there may be a pattern that if a provider has used so many Admit Diagnosis Codes then it might increases or decreases the chances of fraud.\n",
    "\n",
    "    train_iobp_df['PRV_Tot_Admit_DCodes'] = train_iobp_df.groupby(['Provider'])['ClmAdmitDiagnosisCode'].transform('nunique')\n",
    "\n",
    "    # ### **Adding `New Feature - 9` :: `Total Unique Number of Diagnosis Group Codes used by a PROVIDER`**\n",
    "    #     * **`Reasoning`** :: The idea behind adding this feature is to see how many unique `Diagnosis Group Codes` used by the Provider.\n",
    "    #        * As there may be a pattern that if a provider has used so many Diagnosis Group Codes then it might increases or decreases the chances of fraud.\n",
    "\n",
    "    train_iobp_df['PRV_Tot_DGrpCodes'] = train_iobp_df.groupby(['Provider'])['DiagnosisGroupCode'].transform('nunique')\n",
    "\n",
    "    ### **Adding `New Feature - 10` :: `Total unique Date of Birth years of beneficiaries provided by a Provider`**\n",
    "\n",
    "    train_iobp_df['DOB_Year'] = train_iobp_df['DOB'].dt.year\n",
    "    train_iobp_df['PRV_Tot_Unq_DOB_Years'] = train_iobp_df.groupby(['Provider'])['DOB_Year'].transform('nunique')\n",
    "    train_iobp_df.drop(['DOB_Year'], axis=1, inplace=True)\n",
    "\n",
    "    ### **Adding `New Feature - 11` :: `Sum of patients age treated by a Provider`**\n",
    "\n",
    "    train_iobp_df['PRV_Bene_Age_Sum'] = train_iobp_df.groupby(['Provider'])['Bene_Age'].transform('sum')\n",
    "\n",
    "    ### **Adding `New Feature - 12` :: `Sum of Insc Claim Re-Imb Amount for a Provider`**\n",
    "\n",
    "    train_iobp_df['PRV_Insc_Clm_ReImb_Amt'] = train_iobp_df.groupby(['Provider'])['InscClaimAmtReimbursed'].transform('sum')\n",
    "\n",
    "    ### **Adding `New Feature - 13` :: `Total number of RKD Patients seen by a Provider`**\n",
    "\n",
    "    train_iobp_df['RenalDiseaseIndicator'] = train_iobp_df['RenalDiseaseIndicator'].apply(lambda val: 1 if val == \"Y\" else 0)\n",
    "\n",
    "    train_iobp_df['PRV_Tot_RKD_Patients'] = train_iobp_df.groupby(['Provider'])['RenalDiseaseIndicator'].transform('sum')\n",
    "\n",
    "\n",
    "    # Dropping these 2 columns as there 99% of values are same\n",
    "    train_iobp_df.drop(['NoOfMonths_PartACov', 'NoOfMonths_PartBCov'], axis=1, inplace=True)\n",
    "\n",
    "    # Filling null values in Admitted_Duration with 0 (as it will represent the patients were admitted for 0 days)\n",
    "    train_iobp_df['Admitted_Duration'].fillna(value=0,inplace=True)\n",
    "    ### **Adding `Aggregated Features` :: For every possible level**\n",
    "\n",
    " # ###  **`Reasoning`** :: The idea behind adding the aggregated features at different levels is that fraud can be done by an individual or group of individuals or entities involved in the claim process.#####''''''\n",
    "\n",
    "    # PRV Aggregate features\n",
    "    train_iobp_df[\"PRV_CoPayment\"] = train_iobp_df.groupby('Provider')['DeductibleAmtPaid'].transform('sum')\n",
    "    train_iobp_df[\"PRV_IP_Annual_ReImb_Amt\"] = train_iobp_df.groupby('Provider')['IPAnnualReimbursementAmt'].transform('sum')\n",
    "    train_iobp_df[\"PRV_IP_Annual_Ded_Amt\"] = train_iobp_df.groupby('Provider')['IPAnnualDeductibleAmt'].transform('sum')\n",
    "    train_iobp_df[\"PRV_OP_Annual_ReImb_Amt\"] = train_iobp_df.groupby('Provider')['OPAnnualReimbursementAmt'].transform('sum')\n",
    "    train_iobp_df[\"PRV_OP_Annual_Ded_Amt\"] = train_iobp_df.groupby('Provider')['OPAnnualDeductibleAmt'].transform('sum')\n",
    "    train_iobp_df[\"PRV_Admit_Duration\"] = train_iobp_df.groupby('Provider')['Admitted_Duration'].transform('sum')\n",
    "    train_iobp_df[\"PRV_Claim_Duration\"] = train_iobp_df.groupby('Provider')['Claim_Duration'].transform('sum')    \n",
    " \n",
    "    \n",
    "    return train_iobp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68029413",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object does not support item assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_iobp_df \u001b[38;5;241m=\u001b[39m \u001b[43mfeature_Enginerring\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_bene_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_ip_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_op_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_tgt_lbls_df\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36mfeature_Enginerring\u001b[1;34m(train_bene_df, train_ip_df, train_op_df, train_tgt_lbls_df)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeature_Enginerring\u001b[39m(train_bene_df,train_ip_df,train_op_df,train_tgt_lbls_df):\n\u001b[1;32m----> 3\u001b[0m     \u001b[43mtrain_ip_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAdmitted?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m      4\u001b[0m     train_op_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdmitted?\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# Merging the IP and OP dataset on the basis of common columns.\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object does not support item assignment"
     ]
    }
   ],
   "source": [
    "train_iobp_df = feature_Enginerring(train_bene_df,train_ip_df,train_op_df,train_tgt_lbls_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91d8949",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iobp_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0cd225",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputCollection = Helpers.createOutputCollection(context)\n",
    "out = Helpers.createTemplateOutputDataset(context=context, outputName=outputDataset, dataFrame=train_iobp_df)\n",
    "outputCollection.addTemplateOutput(out)\n",
    "Helpers.save(context)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
